"""LLM chat cog with conversation history, image support, and tool execution."""

from __future__ import annotations

import asyncio
import base64
import functools
import io
import json
import logging
import os
import random
import re
import textwrap
import time
from collections import deque
from contextlib import suppress
from dataclasses import dataclass, field
from pathlib import Path
from typing import Deque, Sequence

import discord
from discord.ext import commands
from PIL import Image

try:
    import cairosvg  # type: ignore
except Exception:
    cairosvg = None  # type: ignore

from celery.result import AsyncResult

from hollingsbot.prompt_db import (
    decrement_temp_bot_replies,
    delete_temp_bot,
    get_temp_bot_by_webhook_id,
    get_temp_bots_for_channel,
)
from hollingsbot.settings import clear_system_prompt_cache, get_default_system_prompt
from hollingsbot.tasks import generate_llm_chat_response
from hollingsbot.tools import get_tool_definitions_text
from hollingsbot.tools.notebook import initialize_notebook, get_notebook_manager
from hollingsbot.tools.parser import execute_tool_call, parse_tool_calls
from hollingsbot.url_metadata import (
    download_images_from_metadata,
    extract_url_metadata,
    format_metadata_for_llm,
)
from hollingsbot.utils.discord_utils import get_display_name

_LOG = logging.getLogger(__name__)

# Regex patterns
_SVG_BLOCK_RE = re.compile(r"<svg\b[^>]*>.*?</svg>", re.IGNORECASE | re.DOTALL)
_CODE_BLOCK_RE = re.compile(r"```(?P<lang>[A-Za-z0-9_+\-]*)\n(?P<body>.*?)(```)$", re.DOTALL | re.MULTILINE)

# File extension mappings
_TEXT_ATTACHMENT_EXTENSIONS = {
    ".txt", ".md", ".json", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".log",
    ".py", ".js", ".ts", ".tsx", ".java", ".go", ".rb", ".rs", ".c", ".cpp",
    ".h", ".hpp", ".cs", ".php", ".css", ".html", ".sql", ".sh", ".bat",
    ".ps1", ".xml", ".csv",
}
_IMAGE_EXTENSIONS = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"}

_CODE_EXTENSION_MAP = {
    "": "txt", "text": "txt", "plaintext": "txt",
    "py": "py", "python": "py",
    "ts": "ts", "tsx": "tsx",
    "js": "js", "javascript": "js",
    "json": "json",
    "yaml": "yml", "yml": "yml",
    "bash": "sh", "sh": "sh", "shell": "sh",
    "go": "go",
    "rs": "rs", "rust": "rs",
    "java": "java",
    "c": "c", "cpp": "cpp", "c++": "cpp",
    "html": "html", "css": "css", "sql": "sql", "xml": "xml", "php": "php",
    "rb": "rb", "ruby": "rb",
    "cs": "cs",
}

# Size limits
_MAX_TEXT_ATTACHMENT_BYTES = 120_000
_IMAGE_MAX_EDGE = 2048
_IMAGE_MAX_BYTES = 9_500_000
_MESSAGE_CHUNK = 1900


@dataclass(slots=True)
class ImageAttachment:
    name: str
    url: str | None
    data_url: str | None
    width: int | None = None
    height: int | None = None
    size: int | None = None

    def clone(self) -> "ImageAttachment":
        return ImageAttachment(
            name=self.name,
            url=self.url,
            data_url=self.data_url,
            width=self.width,
            height=self.height,
            size=self.size,
        )

    def to_payload(self) -> dict[str, object]:
        return {
            "name": self.name,
            "url": self.url,
            "data_url": self.data_url,
            "width": self.width,
            "height": self.height,
            "size": self.size,
        }


@dataclass(slots=True)
class ConversationTurn:
    role: str
    content: str
    images: list[ImageAttachment] = field(default_factory=list)
    message_id: int | None = None
    author_id: int | None = None
    author_name: str | None = None
    webhook_id: int | None = None


@dataclass(slots=True)
class ModelTurn:
    role: str
    text: str
    images: list[ImageAttachment] = field(default_factory=list)


@dataclass
class GenerationJob:
    task: asyncio.Task[None] | None = None
    result: AsyncResult | None = None


class LLMChatNewCog(commands.Cog):
    """
    LLM-powered chat cog with conversation history, image support, and tool execution.

    Features:
    - Per-channel conversation history with configurable limits
    - Text and image attachment support
    - URL metadata extraction (Open Graph, Twitter Card)
    - SVG rendering to PNG
    - Tool execution capabilities
    - Per-user model preferences
    - Customizable system prompts
    """

    def __init__(self, bot: commands.Bot) -> None:
        self.bot = bot
        self.whitelist_channels = self._parse_channel_ids(os.getenv("LLM_WHITELIST_CHANNELS", ""))
        self.history_limit = max(1, int(os.getenv("LLM_HISTORY_LIMIT", "50")))
        self.max_turns_sent = max(1, int(os.getenv("LLM_MAX_TURNS_SENT", "8")))
        self.text_timeout = float(os.getenv("TEXT_TIMEOUT", "180"))
        self.default_provider = os.getenv("DEFAULT_LLM_PROVIDER", "openai").strip().lower() or "openai"
        self.default_model = os.getenv("DEFAULT_LLM_MODEL", "gpt-4o").strip() or "gpt-4o"
        self.available_models = self._load_available_models()
        self._model_lookup = {
            (provider.lower(), model): (provider, model)
            for provider, model in self.available_models
        }
        if (self.default_provider, self.default_model) not in {
            (p.lower(), m) for p, m in self.available_models
        }:
            self.available_models.append((self.default_provider, self.default_model))
            self._model_lookup[(self.default_provider, self.default_model)] = (
                self.default_provider,
                self.default_model,
            )

        self.state_path = Path("generated") / "llm_chat_new_state.json"
        self.state_path.parent.mkdir(parents=True, exist_ok=True)
        self._state = self._load_state()

        # Initialize notebook
        notebook_path = Path("generated") / "notebook_state.json"
        initialize_notebook(notebook_path)

        stored_prompt = self._state.get("system_prompt") if isinstance(self._state, dict) else None
        self.base_system_prompt = (
            str(stored_prompt)
            if stored_prompt
            else get_default_system_prompt()
        )

        # Build full system prompt with notebook and tool definitions
        self.system_prompt = self._build_full_system_prompt(self.base_system_prompt)
        raw_prefs = self._state.get("model_preferences") if isinstance(self._state, dict) else {}
        if isinstance(raw_prefs, dict):
            self.model_preferences: dict[str, dict[str, dict[str, str]]] = raw_prefs
        else:
            self.model_preferences = {}

        self.channel_histories: dict[int, Deque[ConversationTurn]] = {}
        self._history_locks: dict[int, asyncio.Lock] = {}
        self._warmed_channels: set[int] = set()
        self._active_generations: dict[int, GenerationJob] = {}

    # ------------------------------------------------------------ state management
    def _build_full_system_prompt(self, base_prompt: str) -> str:
        """Build the complete system prompt with notebook and tool definitions.

        Args:
            base_prompt: The base system prompt text

        Returns:
            Full system prompt including notebook and tools
        """
        parts = [base_prompt]

        # Add notebook contents
        notebook_manager = get_notebook_manager()
        if notebook_manager:
            notebook_text = notebook_manager.get_notebook_text()
            if notebook_text:
                parts.append(notebook_text)

        # Add tool definitions
        tool_definitions = get_tool_definitions_text()
        if tool_definitions:
            parts.append(tool_definitions)

        return "\n\n".join(parts)

    @staticmethod
    def _parse_channel_ids(raw: str) -> set[int]:
        """Parse comma-separated channel IDs from environment variable."""
        values: set[int] = set()
        for token in raw.split(","):
            token = token.strip()
            if token.isdigit():
                values.add(int(token))
        return values

    def _load_state(self) -> dict[str, object]:
        """Load persisted state (system prompt, model preferences) from disk."""
        if not self.state_path.exists():
            return {}
        try:
            text = self.state_path.read_text("utf-8")
            data = json.loads(text)
            if isinstance(data, dict):
                return data
        except Exception:  # noqa: BLE001
            _LOG.exception("Failed to load %s", self.state_path)
        return {}

    def _save_state(self) -> None:
        """Save state (base system prompt, model preferences) to disk atomically."""
        payload = {
            "system_prompt": self.base_system_prompt,
            "model_preferences": self.model_preferences,
        }
        tmp_path = self.state_path.with_suffix(".tmp")
        tmp_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), "utf-8")
        tmp_path.replace(self.state_path)

    def _load_available_models(self) -> list[tuple[str, str]]:
        """Load available models from AVAILABLE_MODELS environment variable."""
        raw = os.getenv("AVAILABLE_MODELS", "")
        models: list[tuple[str, str]] = []
        for token in raw.split(","):
            token = token.strip()
            if not token:
                continue
            if "/" in token:
                provider, model = token.split("/", 1)
            else:
                provider, model = self.default_provider, token
            provider = provider.strip().lower()
            model = model.strip()
            if provider and model:
                models.append((provider, model))
        if not models:
            models.append((self.default_provider, self.default_model))
        return models

    # ------------------------------------------------- history management
    def _history_for_channel(self, channel_id: int) -> Deque[ConversationTurn]:
        """Get or create conversation history for a channel."""
        history = self.channel_histories.get(channel_id)
        if history is None or history.maxlen != self.history_limit:
            existing = list(history) if history else []
            history = deque(existing, maxlen=self.history_limit)
            self.channel_histories[channel_id] = history
        return history

    def _lock_for_channel(self, channel_id: int) -> asyncio.Lock:
        """Get or create lock for thread-safe history access."""
        lock = self._history_locks.get(channel_id)
        if lock is None:
            lock = asyncio.Lock()
            self._history_locks[channel_id] = lock
        return lock

    def _clean_mentions(self, message: discord.Message) -> str:
        """Replace user mentions with display names instead of IDs."""
        content = message.content
        if not content:
            return ""

        # Build a mapping of user IDs to display names from message.mentions
        user_map: dict[int, str] = {}
        for user in message.mentions:
            user_map[user.id] = user.display_name

        # Replace user mentions <@123456> or <@!123456> with @displayname
        def replace_user_mention(match: re.Match[str]) -> str:
            user_id_str = match.group(1)
            user_id = int(user_id_str)

            # Use the pre-built map from message.mentions
            if user_id in user_map:
                return f"@{user_map[user_id]}"

            # Fallback: try guild member lookup
            if message.guild:
                member = message.guild.get_member(user_id)
                if member:
                    return f"@{member.display_name}"

            # Fallback: try bot's user cache
            user = self.bot.get_user(user_id)
            if user:
                return f"@{user.display_name}"

            # Last resort - return @Unknown or keep the mention
            return f"@User{user_id_str}"

        # Pattern matches <@123456> or <@!123456>
        content = re.sub(r'<@!?(\d+)>', replace_user_mention, content)

        return content

    # --------------------------------------------------- message filtering
    def _channel_allowed(self, channel: discord.abc.MessageableChannel, mentioned: bool = False) -> bool:
        """Check if LLM chat is enabled for this channel.

        Args:
            channel: The channel to check
            mentioned: Whether the bot was mentioned in the message

        Returns:
            True if the bot should respond in this channel
        """
        channel_id = getattr(channel, "id", None)
        if channel_id is None:
            return False

        # If bot is mentioned, allow response in any channel
        if mentioned:
            return True

        if not self.whitelist_channels:
            # Empty whitelist disables the feature entirely
            return False
        return channel_id in self.whitelist_channels

    @staticmethod
    def _should_ignore_message(content: str | None) -> bool:
        """Check if message should be ignored (bot commands or image generation)."""
        if not content:
            return False
        stripped = content.lstrip()
        if not stripped:
            return False
        lowered = stripped.lower()
        return stripped.startswith("!") or stripped.startswith("-") or lowered.startswith("edit:")

    # -------------------------------------------------------- attachment detection
    def _is_text_attachment(self, attachment: discord.Attachment) -> bool:
        """Check if attachment is a text file."""
        if attachment.size == 0:
            return False
        if attachment.content_type:
            ctype = attachment.content_type.lower()
            if ctype.startswith("text/"):
                return True
            if ctype in {
                "application/json",
                "application/javascript",
                "application/xml",
                "application/x-yaml",
            }:
                return True
        _, ext = os.path.splitext(attachment.filename)
        return ext.lower() in _TEXT_ATTACHMENT_EXTENSIONS

    def _is_image_attachment(self, attachment: discord.Attachment) -> bool:
        """Check if attachment is an image file."""
        if attachment.content_type:
            if attachment.content_type.lower().startswith("image/"):
                return True
        _, ext = os.path.splitext(attachment.filename)
        return ext.lower() in _IMAGE_EXTENSIONS

    # ------------------------------------------------------- text attachments
    async def _read_text_attachment(
        self, attachment: discord.Attachment
    ) -> tuple[str, bool] | None:
        """Read text attachment and return (content, was_truncated) or None on error."""
        try:
            data = await attachment.read()
        except Exception:
            _LOG.exception("Failed to read text attachment %s", attachment.filename)
            return None

        truncated = len(data) > _MAX_TEXT_ATTACHMENT_BYTES
        if truncated:
            data = data[:_MAX_TEXT_ATTACHMENT_BYTES]

        text = data.decode("utf-8", errors="replace")
        return text, truncated

    async def _collect_text_attachments_full(
        self, message: discord.Message
    ) -> tuple[list[str], list[str]]:
        """Collect text attachments returning (full_blocks, placeholders)."""
        full_blocks: list[str] = []
        placeholders: list[str] = []

        for attachment in message.attachments:
            if not self._is_text_attachment(attachment):
                continue

            result = await self._read_text_attachment(attachment)
            if result is None:
                continue

            text, truncated = result
            block = f"[begin uploaded file: {attachment.filename}]\n{text}\n[end uploaded file]"
            if truncated:
                block += "\n[truncated]"
            full_blocks.append(block)

            placeholder = f"[uploaded file {attachment.filename} removed]"
            if truncated:
                placeholder += " (truncated)"
            placeholders.append(placeholder)

        return full_blocks, placeholders

    async def _collect_text_placeholders(self, message: discord.Message) -> list[str]:
        """Collect placeholders for text attachments (for history)."""
        placeholders: list[str] = []
        for attachment in message.attachments:
            if self._is_text_attachment(attachment):
                placeholders.append(f"[uploaded file {attachment.filename} removed]")
        return placeholders

    # ------------------------------------------------------ image attachments
    def _encode_jpeg(self, image: Image.Image) -> bytes:
        """Encode image as JPEG, trying progressively lower quality to meet size limit."""
        for quality in (90, 85, 80, 75, 70, 60, 50):
            out = io.BytesIO()
            image.save(out, format="JPEG", optimize=True, quality=quality)
            if out.tell() <= _IMAGE_MAX_BYTES:
                return out.getvalue()
        return out.getvalue()

    def _resize_image_if_needed(self, img: Image.Image) -> Image.Image:
        """Resize image if it exceeds maximum edge length."""
        width, height = img.size
        longest = max(width, height)
        if longest <= _IMAGE_MAX_EDGE:
            return img

        scale = _IMAGE_MAX_EDGE / float(longest)
        new_size = (max(1, int(width * scale)), max(1, int(height * scale)))
        return img.resize(new_size, Image.LANCZOS)

    async def _prepare_image_attachment(
        self, attachment: discord.Attachment
    ) -> ImageAttachment | None:
        """Download and process image attachment into ImageAttachment object."""
        try:
            data = await attachment.read()
        except Exception:
            _LOG.exception("Failed to download image attachment %s", attachment.filename)
            return None

        try:
            with Image.open(io.BytesIO(data)) as img:
                img = img.convert("RGB")
                img = self._resize_image_if_needed(img)
                width, height = img.size
                jpeg_bytes = self._encode_jpeg(img)
        except Exception:
            _LOG.exception("Failed to process image attachment %s", attachment.filename)
            return ImageAttachment(
                name=attachment.filename,
                url=attachment.url,
                data_url=None,
                width=None,
                height=None,
                size=attachment.size,
            )

        data_url = "data:image/jpeg;base64," + base64.b64encode(jpeg_bytes).decode("ascii")
        return ImageAttachment(
            name=attachment.filename,
            url=attachment.url,
            data_url=data_url,
            width=width,
            height=height,
            size=len(jpeg_bytes),
        )

    def _image_from_bytes(self, name: str, data: bytes) -> ImageAttachment:
        """Create ImageAttachment from raw bytes (e.g., SVG conversion)."""
        try:
            with Image.open(io.BytesIO(data)) as img:
                width, height = img.size
        except Exception:
            width = height = None
        data_url = "data:image/png;base64," + base64.b64encode(data).decode("ascii")
        return ImageAttachment(
            name=name, url=None, data_url=data_url, width=width, height=height, size=len(data)
        )

    def _images_from_history(
        self, channel_id: int, message_id: int | None
    ) -> list[ImageAttachment]:
        """Retrieve images from a previous message in channel history."""
        if message_id is None:
            return []
        history = self.channel_histories.get(channel_id)
        if not history:
            return []
        for turn in reversed(history):
            if turn.message_id == message_id:
                return [img.clone() for img in turn.images]
        return []

    async def _collect_image_attachments(
        self, message: discord.Message
    ) -> list[ImageAttachment]:
        """Collect all image attachments from a message."""
        images: list[ImageAttachment] = []
        for attachment in message.attachments:
            if not self._is_image_attachment(attachment):
                continue
            img = await self._prepare_image_attachment(attachment)
            if img:
                images.append(img)
        return images

    # -------------------------------------------------------- reply context
    async def _fetch_referenced_message(
        self, message: discord.Message
    ) -> discord.Message | None:
        """Fetch the message being replied to, if any."""
        ref = message.reference
        if not ref or not ref.message_id:
            return None

        resolved = ref.resolved if isinstance(ref.resolved, discord.Message) else None
        if resolved:
            return resolved

        try:
            return await message.channel.fetch_message(ref.message_id)
        except Exception:
            _LOG.exception("Failed to fetch referenced message %s", ref.message_id)
            return None

    async def _build_reply_hint(
        self, message: discord.Message
    ) -> tuple[str | None, list[ImageAttachment]]:
        """Build reply hint text and collect images from referenced message."""
        ref_message = await self._fetch_referenced_message(message)
        if ref_message is None:
            return None, []

        # Get display name with proper fallback: server nick > global display > username
        if ref_message.author:
            display = get_display_name(ref_message.author)
        else:
            display = "Unknown"

        snippet = self._clean_mentions(ref_message).strip()

        if snippet:
            snippet = textwrap.shorten(snippet.replace("\n", " "), width=140, placeholder="â€¦")
            hint = f"(Replying to <{display}>: {snippet})"
        else:
            hint = f"(Replying to <{display}>.)"

        images = self._images_from_history(message.channel.id, ref_message.id)
        if not images:
            images = await self._collect_image_attachments(ref_message)

        return hint, [img.clone() for img in images]

    # ----------------------------------------------------- turn building
    def _build_user_message_text(
        self, display_name: str, reply_hint: str | None, base_text: str
    ) -> str:
        """Build formatted user message text with display name and optional reply hint."""
        body_parts: list[str] = []
        if reply_hint:
            body_parts.append(reply_hint)
        if base_text:
            body_parts.append(base_text)

        body = "\n".join(part for part in body_parts if part).strip()
        if not body:
            body = "[no content]"

        return f"<{display_name}>: {body}"

    async def _extract_url_images(
        self, base_text: str
    ) -> tuple[list[ImageAttachment], str, str]:
        """
        Extract URL metadata and images from text.
        Returns (images, full_metadata_text, history_metadata_text).
        """
        url_images: list[ImageAttachment] = []
        full_metadata_text = ""
        history_metadata_text = ""

        if not base_text:
            return url_images, full_metadata_text, history_metadata_text

        url_metadata_list = await extract_url_metadata(base_text)
        if not url_metadata_list:
            return url_images, full_metadata_text, history_metadata_text

        # For current turn to LLM: include images
        full_metadata_parts = [
            format_metadata_for_llm(m, include_images=True) for m in url_metadata_list
        ]
        full_metadata_text = "\n\n".join(full_metadata_parts)

        # For history: exclude images
        history_metadata_parts = [
            format_metadata_for_llm(m, include_images=False) for m in url_metadata_list
        ]
        history_metadata_text = "\n\n".join(history_metadata_parts)

        # Download and process images from URL metadata
        for metadata in url_metadata_list:
            downloaded_images = await download_images_from_metadata(metadata)
            # Convert url_metadata.ImageAttachment to llm_chat.ImageAttachment
            for url_img in downloaded_images:
                url_images.append(
                    ImageAttachment(
                        name=url_img.name,
                        url=url_img.url,
                        data_url=url_img.data_url,
                        width=url_img.width,
                        height=url_img.height,
                        size=url_img.size,
                    )
                )

        return url_images, full_metadata_text, history_metadata_text

    async def _prepare_user_turn(
        self, message: discord.Message
    ) -> tuple[ModelTurn, ConversationTurn]:
        """Prepare user turn with full content for LLM and lighter version for history."""
        display = get_display_name(message.author)
        hint, reply_images = await self._build_reply_hint(message)
        base_text = self._clean_mentions(message).strip()

        prefixed = self._build_user_message_text(display, hint, base_text)

        # Collect text attachments
        text_blocks, placeholders = await self._collect_text_attachments_full(message)
        full_text = prefixed
        for block in text_blocks:
            full_text += f"\n{block}"

        history_text = prefixed
        for placeholder in placeholders:
            history_text += f"\n{placeholder}"

        # Extract and append URL metadata
        url_images, full_metadata, history_metadata = await self._extract_url_images(base_text)
        if full_metadata:
            full_text += f"\n\n{full_metadata}"
        if history_metadata:
            history_text += f"\n\n{history_metadata}"

        # Merge all images
        current_images = await self._collect_image_attachments(message)
        merged_images = [img.clone() for img in reply_images + current_images + url_images]

        model_turn = ModelTurn(
            role="user", text=full_text, images=[img.clone() for img in merged_images]
        )
        history_turn = ConversationTurn(
            role="user",
            content=history_text,
            images=merged_images,
            message_id=message.id,
            author_id=message.author.id,
            author_name=display,
            webhook_id=message.webhook_id,
        )
        return model_turn, history_turn

    async def _build_history_user_turn(
        self, message: discord.Message
    ) -> ConversationTurn | None:
        """Build lightweight user turn for preloading history."""
        if self._should_ignore_message(message.content):
            return None

        display = get_display_name(message.author)
        hint, reply_images = await self._build_reply_hint(message)
        base_text = self._clean_mentions(message).strip()

        text = self._build_user_message_text(display, hint, base_text)

        # Add text attachment placeholders
        placeholders = await self._collect_text_placeholders(message)
        for placeholder in placeholders:
            text += f"\n{placeholder}"

        images = reply_images + await self._collect_image_attachments(message)
        return ConversationTurn(
            role="user",
            content=text,
            images=images,
            message_id=message.id,
            author_id=message.author.id,
            author_name=display,
            webhook_id=message.webhook_id,
        )

    async def _build_assistant_turn_from_message(
        self, message: discord.Message
    ) -> ConversationTurn:
        """Build assistant turn from bot's own message in history."""
        text = (message.content or "").strip() or "[no content]"
        placeholders = await self._collect_text_placeholders(message)
        for placeholder in placeholders:
            text += f"\n{placeholder}"
        images = await self._collect_image_attachments(message)
        if message.author:
            display = get_display_name(message.author)
        else:
            display = "Bot"
        return ConversationTurn(
            role="assistant",
            content=text,
            images=images,
            message_id=message.id,
            author_id=message.author.id if message.author else None,
            author_name=display,
            webhook_id=message.webhook_id,
        )

    # ------------------------------------------------ conversation building
    def _build_conversation_payload(
        self,
        history: Sequence[ConversationTurn],
        current_turn: ModelTurn,
        responding_webhook_id: int | None = None,
    ) -> list[dict[str, object]]:
        """Build conversation payload for LLM, including system prompt and recent history.

        Args:
            history: Conversation history
            current_turn: The current user turn to respond to
            responding_webhook_id: The webhook_id of the bot responding (None for main bot)
                This is used to adjust roles so each bot sees other bots' messages as "user"
        """
        recent = list(history)[-self.max_turns_sent :]
        conversation: list[dict[str, object]] = [
            {
                "role": "system",
                "text": self.system_prompt,
                "images": [],
            }
        ]
        for turn in recent:
            # Adjust role based on who is responding:
            # - If turn is from the current responder -> "assistant"
            # - Otherwise (from user or different bot) -> "user"
            if turn.role == "user":
                # User messages are always "user"
                role = "user"
            else:
                # Bot message - check if it's from the current responder
                is_from_current_bot = turn.webhook_id == responding_webhook_id
                role = "assistant" if is_from_current_bot else "user"
                _LOG.info(
                    f"Role adjustment: turn.webhook_id={turn.webhook_id}, "
                    f"responding_webhook_id={responding_webhook_id}, "
                    f"is_from_current_bot={is_from_current_bot}, role={role}, "
                    f"content_preview={turn.content[:50]}"
                )

            conversation.append(
                {
                    "role": role,
                    "text": turn.content,
                    "images": [img.to_payload() for img in turn.images],
                }
            )
        conversation.append(
            {
                "role": current_turn.role,
                "text": current_turn.text,
                "images": [img.to_payload() for img in current_turn.images],
            }
        )
        return conversation

    # ------------------------------------------------- model preferences
    def _is_valid_model(self, provider: str, model: str) -> bool:
        """Check if provider/model combination is available."""
        key = (provider.lower(), model)
        return key in self._model_lookup

    def _get_model_for_user(self, guild_id: int | None, user_id: int) -> tuple[str, str]:
        """Get user's preferred model or return default."""
        gid = str(guild_id or 0)
        uid = str(user_id)
        entry = self.model_preferences.get(gid, {}).get(uid)
        if isinstance(entry, dict):
            provider = entry.get("provider")
            model = entry.get("model")
            if isinstance(provider, str) and isinstance(model, str) and self._is_valid_model(provider, model):
                return provider.lower(), model
        return self.default_provider, self.default_model

    def _set_model_for_user(self, guild_id: int | None, user_id: int, provider: str, model: str) -> None:
        """Save user's model preference."""
        gid = str(guild_id or 0)
        uid = str(user_id)
        guild_entry = self.model_preferences.setdefault(gid, {})
        guild_entry[uid] = {"provider": provider.lower(), "model": model}
        self._save_state()

    # ------------------------------------------------- history warming
    async def _ensure_channel_warm(self, channel: discord.abc.MessageableChannel) -> None:
        """Preload channel history on first message to provide context."""
        channel_id = getattr(channel, "id", None)
        if channel_id is None or channel_id in self._warmed_channels:
            return
        history = self._history_for_channel(channel_id)
        limit = max(self.history_limit, self.max_turns_sent) * 3
        try:
            messages: list[discord.Message] = []
            async for item in channel.history(limit=limit, oldest_first=False):
                messages.append(item)
        except Exception:  # noqa: BLE001
            _LOG.exception("Failed to preload history for channel %s", channel_id)
            self._warmed_channels.add(channel_id)
            return
        # Reverse to get chronological order (oldest to newest)
        messages.reverse()
        for msg in messages:
            if msg.author and msg.author.bot and msg.author != self.bot.user:
                continue
            if msg.author == self.bot.user:
                turn = await self._build_assistant_turn_from_message(msg)
            else:
                turn = await self._build_history_user_turn(msg)
                if turn is None:
                    continue
            history.append(turn)
        self._warmed_channels.add(channel_id)

    # --------------------------------------------------------- utilities
    def _code_extension(self, language: str) -> str:
        """Map code language identifier to file extension."""
        return _CODE_EXTENSION_MAP.get(language.lower(), "txt")

    def _chunk_text(self, text: str) -> list[str]:
        """Split text into chunks that fit Discord's message limit."""
        text = text.strip()
        if len(text) <= _MESSAGE_CHUNK:
            return [text] if text else []
        chunks: list[str] = []
        buffer = ""
        for line in text.splitlines(keepends=True):
            if len(buffer) + len(line) > _MESSAGE_CHUNK:
                if buffer:
                    chunks.append(buffer.rstrip())
                    buffer = ""
                while len(line) > _MESSAGE_CHUNK:
                    chunks.append(line[:_MESSAGE_CHUNK])
                    line = line[_MESSAGE_CHUNK :]
            buffer += line
        if buffer.strip():
            chunks.append(buffer.rstrip())
        return [chunk for chunk in chunks if chunk]

    def _conversation_for_celery(
        self,
        conversation: list[dict[str, object]],
    ) -> list[dict[str, object]]:
        """Normalize conversation entries for Celery task."""
        normalized: list[dict[str, object]] = []
        for entry in conversation:
            normalized.append(
                {
                    "role": entry.get("role", "user"),
                    "text": entry.get("text", ""),
                    "images": entry.get("images", []),
                }
            )
        return normalized

    async def _cancel_generation(self, channel_id: int) -> None:
        """Cancel active generation task for a channel."""
        job = self._active_generations.pop(channel_id, None)
        if not job or not job.task:
            return
        job.task.cancel()
        with suppress(asyncio.CancelledError):
            await job.task

    # ------------------------------------------------------------- core flow
    @commands.Cog.listener()
    async def on_message(self, message: discord.Message) -> None:
        """Handle incoming messages and generate LLM responses."""
        # Track if message is from main bot (to allow temp bots to respond but prevent main bot self-response)
        is_main_bot = message.author == self.bot.user

        # Allow temp bot webhooks, but filter out other bots
        is_temp_bot = False
        if message.author.bot and not is_main_bot:
            if message.webhook_id:
                # Check if this webhook is a registered temp bot
                temp_bot_data = get_temp_bot_by_webhook_id(message.webhook_id)
                if temp_bot_data:
                    is_temp_bot = True
                else:
                    return  # Not a temp bot, ignore
            else:
                return  # Regular bot, ignore

        channel = message.channel

        # Check if bot was explicitly mentioned (not via reply)
        # Replies automatically add the replied-to user to mentions, so we need to filter those out
        bot_mentioned = False
        if self.bot.user in message.mentions:
            # Check if this is a reply mention or an explicit mention
            # If it's a reply and the bot is the author of the replied message, this is a reply mention
            if message.reference and message.reference.resolved:
                replied_message = message.reference.resolved
                if isinstance(replied_message, discord.Message) and replied_message.author == self.bot.user:
                    # This is a reply to the bot's message, not an explicit mention
                    bot_mentioned = False
                else:
                    # Not replying to bot, so this must be an explicit mention
                    bot_mentioned = True
            else:
                # No reply context, so this is an explicit mention
                bot_mentioned = True

        if not self._channel_allowed(channel, mentioned=bot_mentioned):
            return
        if self._should_ignore_message(message.content):
            return

        # Ignore spawn prompt system messages and empty messages
        if message.content.startswith("[Spawn prompt for ") or not message.content.strip():
            return

        await self._ensure_channel_warm(channel)
        provider, model = self._get_model_for_user(
            getattr(message.guild, "id", None), message.author.id
        )
        channel_id = channel.id
        lock = self._lock_for_channel(channel_id)

        async with lock:
            model_turn, history_turn = await self._prepare_user_turn(message)
            history = self._history_for_channel(channel_id)
            snapshot = list(history)
            # Only add to history if NOT from main bot (main bot already added itself)
            if not is_main_bot:
                history.append(history_turn)

        # Bot selection logic
        respond_as_webhook_id: int | None = None
        temp_bot_spawn_prompt: str | None = None
        temp_bot_name: str | None = None
        temp_bots = get_temp_bots_for_channel(channel_id)

        if is_main_bot:
            # Main bot message - only temp bots can respond
            if temp_bots:
                chosen_bot = random.choice(temp_bots)
                respond_as_webhook_id = chosen_bot["webhook_id"]
                temp_bot_spawn_prompt = chosen_bot["spawn_prompt"]
                temp_bot_name = chosen_bot["name"]
                _LOG.info(f"Main bot spoke, temp bot '{temp_bot_name}' will respond")
            else:
                # No temp bots available, don't respond to self
                return
        else:
            # User or temp bot message - randomly choose between main bot and temp bots
            # Exclude the temp bot that just spoke to prevent self-responses
            available_temp_bots = temp_bots
            if is_temp_bot and message.webhook_id:
                speaking_bot_name = next((b["name"] for b in temp_bots if b["webhook_id"] == message.webhook_id), "Unknown")
                available_temp_bots = [bot for bot in temp_bots if bot["webhook_id"] != message.webhook_id]
                _LOG.info(f"Temp bot '{speaking_bot_name}' spoke, excluded from response. {len(available_temp_bots)} temp bots available")

            if available_temp_bots and random.random() < 0.5:  # 50% chance to use temp bot if available
                chosen_bot = random.choice(available_temp_bots)
                respond_as_webhook_id = chosen_bot["webhook_id"]
                temp_bot_spawn_prompt = chosen_bot["spawn_prompt"]
                temp_bot_name = chosen_bot["name"]
                _LOG.info(f"Selected temp bot '{temp_bot_name}' to respond")
            else:
                _LOG.info(f"Selected main bot to respond (is_temp_bot={is_temp_bot}, available_temp_bots={len(available_temp_bots)})")
            # else: respond_as_webhook_id remains None, meaning main bot will respond

        # Build conversation with role adjustments based on who is responding
        conversation = self._build_conversation_payload(snapshot, model_turn, respond_as_webhook_id)

        # If responding as temp bot, replace system prompt with simplified version (no notebook/tools)
        if respond_as_webhook_id and temp_bot_spawn_prompt and temp_bot_name:
            personality_suffix = (
                f"\n\nIMPORTANT: Your main purpose and personality trait is: {temp_bot_spawn_prompt}\n\n"
                f"You are {temp_bot_name}, and this directive defines your core behavior and personality. "
                f"Embody this trait in all your responses."
            )
            # Use base system prompt (without notebook and tools) for temp bots
            conversation[0]["text"] = self.base_system_prompt + personality_suffix

        await self._cancel_generation(channel_id)

        # If responding as temp bot, atomically reserve a reply slot before generation
        if respond_as_webhook_id:
            remaining = decrement_temp_bot_replies(respond_as_webhook_id)
            _LOG.info(f"Reserved reply slot for webhook_id={respond_as_webhook_id}, {remaining} remaining")
            if remaining < 0:
                # Bot was depleted between selection and reservation, don't respond
                _LOG.warning(f"Temp bot webhook_id={respond_as_webhook_id} depleted during reservation")
                return

        job = GenerationJob()
        task = self.bot.loop.create_task(
            self._generate_and_send_response(
                message,
                model_turn,
                self._conversation_for_celery(conversation),
                provider,
                model,
                job,
                respond_as_webhook_id,
            )
        )
        job.task = task
        self._active_generations[channel_id] = job

    # --------------------------------------------------------- tool execution
    def _execute_tool_calls(self, text: str) -> tuple[str, list[str]]:
        """Execute tool calls in response and return (cleaned_text, channel_messages).

        Returns:
            tuple: (text with TOOL_CALL markers removed, list of messages to append to response)
        """
        from hollingsbot.tools import AVAILABLE_TOOLS

        tool_calls = parse_tool_calls(text)
        if not tool_calls:
            return text, []

        _LOG.info("Found %d tool call(s) in response", len(tool_calls))

        channel_messages: list[str] = []

        # Execute all tool calls
        for tool_call in tool_calls:
            result, error = execute_tool_call(tool_call)
            if error:
                _LOG.warning("Tool call failed: %s - %s", tool_call.tool_name, error)
            else:
                _LOG.info("Tool %s executed successfully", tool_call.tool_name)
                # Check if this tool has a channel message
                tool = AVAILABLE_TOOLS.get(tool_call.tool_name)
                if tool and tool.channel_message:
                    channel_messages.append(tool.channel_message)

        # Rebuild system prompt to include any notebook changes
        self.system_prompt = self._build_full_system_prompt(self.base_system_prompt)

        # Remove TOOL_CALL markers from text
        text = self._clean_tool_calls_from_text(text)

        return text, channel_messages

    async def _generate_and_send_response(
        self,
        message: discord.Message,
        current_turn: ModelTurn,
        conversation: list[dict[str, object]],
        provider: str,
        model: str,
        job: GenerationJob,
        webhook_id: int | None = None,
    ) -> None:
        """Generate LLM response, execute tools if needed, and send to channel."""
        channel = message.channel

        try:
            async with channel.typing():
                # Generate response
                text = await self._run_generation(provider, model, conversation, job)

                # Execute any tool calls and get channel messages
                text, tool_messages = self._execute_tool_calls(text)

        except asyncio.CancelledError:
            raise
        except Exception as exc:
            _LOG.exception(
                "Generation failed for channel %s (provider=%s model=%s): %s",
                channel.id,
                provider,
                model,
                exc,
            )
            await channel.send(f"Generation failed: {exc}")
            return
        finally:
            if self._active_generations.get(channel.id) is job:
                self._active_generations.pop(channel.id, None)

        if not text.strip():
            await channel.send("Generation failed: empty response.")
            return

        # Check for <no response> directive
        if "<no response>" in text.lower():
            _LOG.info("LLM chose not to respond in channel %s", channel.id)
            return

        assistant_turn = await self._deliver_response(channel, text, tool_messages, webhook_id)
        assistant_turn.role = "assistant"
        assistant_turn.webhook_id = webhook_id
        lock = self._lock_for_channel(channel.id)
        async with lock:
            history = self._history_for_channel(channel.id)
            # Set a synthetic message id by referencing the most recent Discord message
            if channel.last_message_id:
                assistant_turn.message_id = channel.last_message_id
            history.append(assistant_turn)

        # If this was a temp bot response, check if depleted and clean up
        # Note: Reply was already decremented before generation started (atomic reservation)
        if webhook_id:
            temp_bot_data = get_temp_bot_by_webhook_id(webhook_id)
            if temp_bot_data and temp_bot_data["replies_remaining"] <= 0:
                # Auto-despawn the temp bot
                _LOG.info(f"Auto-despawning temp bot webhook_id={webhook_id} (depleted)")
                try:
                    if isinstance(channel, discord.TextChannel):
                        webhook_obj = await self.bot.fetch_webhook(webhook_id)
                        await webhook_obj.delete(reason="Temp bot depleted all replies")
                        _LOG.info(f"Auto-despawned temp bot webhook_id={webhook_id}")
                except (discord.NotFound, discord.Forbidden):
                    _LOG.warning(f"Failed to delete webhook {webhook_id}, removing from DB anyway")
                finally:
                    delete_temp_bot(webhook_id)

    async def _run_generation(
        self,
        provider: str,
        model: str,
        conversation: list[dict[str, object]],
        job: GenerationJob,
    ) -> str:
        """
        Run a single LLM generation via Celery and return the text.
        Raises TimeoutError if generation exceeds configured timeout.
        """
        async_result = generate_llm_chat_response.apply_async(
            (provider, model, conversation),
            kwargs={"temperature": 1.0},
        )
        job.result = async_result
        start = time.monotonic()
        while True:
            if async_result.ready():
                break
            if (time.monotonic() - start) > self.text_timeout:
                async_result.revoke(terminate=True)
                raise TimeoutError(f"timed out after {self.text_timeout:.0f}s")
            await asyncio.sleep(0.5)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(
            None,
            functools.partial(async_result.get, timeout=0.1),
        )
        return str(result.get("text", "")) if isinstance(result, dict) else str(result)

    # -------------------------------------------------- response delivery
    def _clean_tool_calls_from_text(self, text: str) -> str:
        """Remove TOOL_CALL markers from text."""
        return re.sub(r'TOOL_CALL:\s*\w+\(.*?\)', '', text, flags=re.IGNORECASE)

    def _extract_and_convert_svgs(
        self, text: str
    ) -> tuple[str, list[discord.File], list[ImageAttachment]]:
        """Extract SVG blocks from text (raw or in code blocks), convert to PNG if possible, return (cleaned_text, files, images)."""
        svg_files: list[discord.File] = []
        svg_images: list[ImageAttachment] = []

        def _replace_svg(match: re.Match[str]) -> str:
            svg = match.group(0)
            idx = len(svg_files) + 1
            timestamp = int(time.time())
            png_name = f"svg_{timestamp}_{idx}.png"

            if cairosvg:
                try:
                    png_bytes = cairosvg.svg2png(bytestring=svg.encode("utf-8"))  # type: ignore[arg-type]
                    svg_files.append(discord.File(io.BytesIO(png_bytes), filename=png_name))
                    svg_images.append(self._image_from_bytes(png_name, png_bytes))
                    return f"[see file: {png_name}]"
                except Exception:
                    _LOG.exception("Failed to convert SVG to PNG; using raw SVG file")

            # Fallback to raw SVG
            svg_name = f"svg_{timestamp}_{idx}.svg"
            svg_files.append(discord.File(io.BytesIO(svg.encode("utf-8")), filename=svg_name))
            return f"[see file: {svg_name}]"

        def _replace_svg_code_block(match: re.Match[str]) -> str:
            """Extract SVG from code block and process it."""
            svg_content = match.group("body")
            idx = len(svg_files) + 1
            timestamp = int(time.time())
            png_name = f"svg_{timestamp}_{idx}.png"

            if cairosvg:
                try:
                    png_bytes = cairosvg.svg2png(bytestring=svg_content.encode("utf-8"))  # type: ignore[arg-type]
                    svg_files.append(discord.File(io.BytesIO(png_bytes), filename=png_name))
                    svg_images.append(self._image_from_bytes(png_name, png_bytes))
                    return f"[see file: {png_name}]"
                except Exception:
                    _LOG.exception("Failed to convert SVG code block to PNG; using raw SVG file")

            # Fallback to raw SVG
            svg_name = f"svg_{timestamp}_{idx}.svg"
            svg_files.append(discord.File(io.BytesIO(svg_content.encode("utf-8")), filename=svg_name))
            return f"[see file: {svg_name}]"

        # First, extract SVG from markdown code blocks (```svg ... ```)
        svg_code_block_pattern = re.compile(
            r"```svg\s*\n(?P<body>.*?)```", re.DOTALL | re.IGNORECASE | re.MULTILINE
        )
        cleaned_text = svg_code_block_pattern.sub(_replace_svg_code_block, text)

        # Then, extract raw SVG tags
        cleaned_text = _SVG_BLOCK_RE.sub(_replace_svg, cleaned_text)

        return cleaned_text, svg_files, svg_images

    async def _send_response_to_channel(
        self, channel: discord.abc.MessageableChannel, text: str, svg_files: list[discord.File], webhook_id: int | None = None
    ) -> list[discord.Message]:
        """Send response text and files to channel, handling length limits."""
        sent_messages: list[discord.Message] = []

        # If webhook_id is provided, send via webhook
        webhook: discord.Webhook | None = None
        webhook_name = None
        webhook_avatar_url = None
        if webhook_id:
            temp_bot = get_temp_bot_by_webhook_id(webhook_id)
            if temp_bot and isinstance(channel, discord.TextChannel):
                try:
                    webhook = await self.bot.fetch_webhook(webhook_id)
                    webhook_name = temp_bot["name"]
                    webhook_avatar_url = temp_bot["avatar_url"]
                except (discord.NotFound, discord.Forbidden):
                    _LOG.warning("Failed to fetch webhook %s, falling back to regular send", webhook_id)
                    webhook = None

        # If response is longer than Discord's limit, upload as text file
        if len(text) > 2000:
            timestamp = int(time.time())
            text_file = discord.File(
                io.BytesIO(text.encode("utf-8")), filename=f"response_{timestamp}.txt"
            )
            all_files = svg_files + [text_file]
            if webhook:
                sent = await webhook.send(
                    "Response (see attached file):",
                    files=all_files,
                    username=webhook_name,
                    avatar_url=webhook_avatar_url,
                    wait=True,
                )
            else:
                sent = await channel.send("Response (see attached file):", files=all_files)
            sent_messages.append(sent)
        else:
            # Response fits in a single message
            if not text:
                text = "[no content]"
            if webhook:
                sent = await webhook.send(
                    text,
                    username=webhook_name,
                    avatar_url=webhook_avatar_url,
                    wait=True,
                )
            else:
                sent = await channel.send(text)
            sent_messages.append(sent)
            if svg_files:
                if webhook:
                    sent = await webhook.send(
                        "Generated attachments:",
                        files=svg_files,
                        username=webhook_name,
                        avatar_url=webhook_avatar_url,
                        wait=True,
                    )
                else:
                    sent = await channel.send("Generated attachments:", files=svg_files)
                sent_messages.append(sent)

        return sent_messages

    async def _deliver_response(
        self, channel: discord.abc.MessageableChannel, raw_text: str, tool_messages: list[str], webhook_id: int | None = None
    ) -> ConversationTurn:
        """Process and deliver response to channel, returning conversation turn for history."""
        text = raw_text.strip()

        # Extract and convert SVGs
        text, svg_files, svg_images = self._extract_and_convert_svgs(text)

        # Append tool messages to the response
        if tool_messages:
            if text:
                text += "\n\n" + "\n".join(tool_messages)
            else:
                text = "\n".join(tool_messages)

        # Send to channel
        sent_messages = await self._send_response_to_channel(channel, text, svg_files, webhook_id)

        # Build conversation turn for history
        turn = ConversationTurn(role="assistant", content=text, images=svg_images)
        if sent_messages:
            turn.message_id = sent_messages[-1].id
        return turn

    # --------------------------------------------------------------- commands
    def _format_model_listing(self, current: tuple[str, str] | None = None) -> str:
        """Format available models list, highlighting current selection."""
        current_normalized = None
        if current:
            current_normalized = f"{current[0].lower()}/{current[1]}"
        lines = ["Available models:"]
        for provider, model in sorted(self.available_models):
            token = f"{provider}/{model}"
            if token.lower() == current_normalized:
                lines.append(f"- **{token}** (current)")
            else:
                lines.append(f"- {token}")
        return "\n".join(lines)

    @commands.command(name="models")
    async def models_command(self, ctx: commands.Context) -> None:
        """List all available LLM models."""
        if not self._channel_allowed(ctx.channel):
            return
        await ctx.send(self._format_model_listing())

    @commands.command(name="model")
    async def model_command(self, ctx: commands.Context, *, selection: str | None = None) -> None:
        """View or set your preferred LLM model (format: provider/model)."""
        if not self._channel_allowed(ctx.channel):
            return
        current = self._get_model_for_user(getattr(ctx.guild, "id", None), ctx.author.id)
        if not selection:
            await ctx.send(self._format_model_listing(current))
            return
        candidate = selection.strip()
        if "/" not in candidate:
            await ctx.send(
                "Please provide a model in the format `provider/model`.\n"
                + self._format_model_listing(current)
            )
            return
        provider, model = candidate.split("/", 1)
        provider = provider.strip().lower()
        model = model.strip()
        if not self._is_valid_model(provider, model):
            await ctx.send("Unknown model.\n" + self._format_model_listing(current))
            return
        self._set_model_for_user(getattr(ctx.guild, "id", None), ctx.author.id, provider, model)
        await ctx.send(f"Model preference updated to {provider}/{model}.")

    @commands.command(name="clear")
    async def clear_command(self, ctx: commands.Context) -> None:
        """Clear conversation history for this channel."""
        if not self._channel_allowed(ctx.channel):
            return
        channel_id = ctx.channel.id
        await self._cancel_generation(channel_id)
        lock = self._lock_for_channel(channel_id)
        async with lock:
            self.channel_histories[channel_id] = deque(maxlen=self.history_limit)
        await ctx.send("Cleared saved conversation context for this channel.")

    def _reset_histories(self) -> None:
        """Clear all channel histories and warming state."""
        self.channel_histories.clear()
        self._warmed_channels.clear()

    async def _cancel_all_generations(self) -> None:
        """Cancel all active generation tasks across all channels."""
        pending = list(self._active_generations.items())
        self._active_generations.clear()
        for channel_id, job in pending:
            if not job.task:
                continue
            job.task.cancel()
        await asyncio.gather(*(job.task for _, job in pending if job.task), return_exceptions=True)

    @commands.command(name="system")
    async def system_command(self, ctx: commands.Context, *, text: str | None = None) -> None:
        """View, set, or reset the global system prompt."""
        if not self._channel_allowed(ctx.channel):
            return
        argument = (text or "").strip()
        if not argument:
            await self._send_system_prompt(ctx)
            return
        if argument.lower() == "reset":
            clear_system_prompt_cache()
            default_prompt = get_default_system_prompt()
            new_full_prompt = self._build_full_system_prompt(default_prompt)
            if self.base_system_prompt == default_prompt:
                await ctx.send("System prompt already matches the default.")
                await self._send_system_prompt(ctx)
                return
            self.base_system_prompt = default_prompt
            self.system_prompt = new_full_prompt
        else:
            new_full_prompt = self._build_full_system_prompt(argument)
            if self.base_system_prompt == argument:
                await ctx.send("System prompt unchanged (new text matches existing value).")
                await self._send_system_prompt(ctx)
                return
            self.base_system_prompt = argument
            self.system_prompt = new_full_prompt
        self._save_state()
        await self._cancel_all_generations()
        self._reset_histories()
        await ctx.send("System prompt updated and chat history cleared.")
        await self._send_system_prompt(ctx)

    async def _send_system_prompt(self, ctx: commands.Context) -> None:
        """Send current system prompt as a file attachment."""
        data = io.StringIO(self.system_prompt)
        file = discord.File(data, filename="system_prompt.txt")
        await ctx.send("Current global system prompt is attached.", file=file)

    @commands.command(name="h")
    async def help_command(self, ctx: commands.Context) -> None:
        """Display help information for LLM chat features."""
        if not self._channel_allowed(ctx.channel):
            return
        lines = [
            "LLM chat quick help:",
            "- Conversation is available only in whitelisted channels.",
            "- Speak normally; avoid leading `!`, `-`, or `edit:` unless you intend a command.",
            "- `!system`, `!system <text>`, and `!system reset` manage the shared system prompt.",
            "- `!clear` wipes the current channel's saved chat context if you need a fresh start.",
            "- `!models` lists options; `!model provider/model` sets your per-guild default.",
            "- Text file attachments are inlined with markers; history keeps lightweight placeholders.",
            "- Images are resized for OpenAI and stay attached to the conversation history.",
            "- Responses show typing, split long messages, convert SVGs to PNG, and attach code blocks when needed.",
            "- `Generation failed: ...` indicates provider or timeout issues; resend your message to retry.",
        ]
        await ctx.send("\n".join(lines))

    # -------------------------------------------------------------- lifecycle
    async def cog_unload(self) -> None:
        """Clean up active tasks when cog is unloaded."""
        await self._cancel_all_generations()


async def setup(bot: commands.Bot) -> None:
    await bot.add_cog(LLMChatNewCog(bot))
